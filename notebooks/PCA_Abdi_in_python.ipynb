{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will reproduce all PCA results of [this paper](https://onlinelibrary.wiley.com/doi/full/10.1002/wics.101) in python. But we will not reproduce figures of that paper here. It is quite difficult to produce exact plots as that of the paper using python. But it is very easy using `ggplot2` package of `R`. Though there are some packages in python that produce `ggplot` like plots, we will not use those. Rather we refer the readers to [this blog post](https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/) where figures have already been reproduced using `R`'s `ggplot2` package.\n",
    "\n",
    "Please note that this post is only a collection of results. To know more about the theory, readers can either read [the paper](https://onlinelibrary.wiley.com/doi/full/10.1002/wics.101) or [the blog](https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-i/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Word_length</th>\n",
       "      <th>Lines_in_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bag</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Across</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insane</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>By</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MOnastery</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Relief</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Slope</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scoundrel</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>With</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Neither</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Pretentious</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Solid</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>For</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Therefore</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Generality</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Arise</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Blot</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Infectious</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Words  Word_length  Lines_in_dict\n",
       "0           Bag            3             14\n",
       "1        Across            6              7\n",
       "2            On            2             11\n",
       "3        Insane            6              9\n",
       "4            By            2              9\n",
       "5     MOnastery            9              4\n",
       "6        Relief            6              8\n",
       "7         Slope            5             11\n",
       "8     Scoundrel            9              5\n",
       "9          With            4              8\n",
       "10      Neither            7              2\n",
       "11  Pretentious           11              4\n",
       "12        Solid            5             12\n",
       "13         This            4              9\n",
       "14          For            3              8\n",
       "15    Therefore            9              1\n",
       "16   Generality           10              4\n",
       "17        Arise            5             13\n",
       "18         Blot            4             15\n",
       "19   Infectious           10              6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = pd.read_csv('D:/rfiles/data/pca_abdi/pca_abdi_words.csv')\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change data types from int to float. This will facilitate our analysis later. Otherwise some warnings will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Words             object\n",
       "Word_length      float64\n",
       "Lines_in_dict    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.iloc[:,1:3] = words.iloc[:,1:3].astype(np.float64)\n",
    "words.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s PCA centers data by default before applying SVD. In this example, we will explicitly center the data before giving it as an argument to PCA's `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.,  6.],\n",
       "       [ 0., -1.],\n",
       "       [-4.,  3.],\n",
       "       [ 0.,  1.],\n",
       "       [-4.,  1.],\n",
       "       [ 3., -4.],\n",
       "       [ 0.,  0.],\n",
       "       [-1.,  3.],\n",
       "       [ 3., -3.],\n",
       "       [-2.,  0.],\n",
       "       [ 1., -6.],\n",
       "       [ 5., -4.],\n",
       "       [-1.,  4.],\n",
       "       [-2.,  1.],\n",
       "       [-3.,  0.],\n",
       "       [ 3., -7.],\n",
       "       [ 4., -4.],\n",
       "       [-1.,  5.],\n",
       "       [-2.,  7.],\n",
       "       [ 4., -2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cent_data = StandardScaler(with_mean= True, with_std = False)\n",
    "centered_words = cent_data.fit_transform(words.iloc[:,1:3])\n",
    "centered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance PCA\n",
    "Covariance PCA uses centered data matrix. But data matrix is not scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_pca = PCA().fit(centered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.67 -0.69]\n",
      " [ 0.84  0.54]\n",
      " [-4.68  1.76]\n",
      " [-0.84 -0.54]\n",
      " [-2.99  2.84]\n",
      " [ 4.99 -0.38]\n",
      " [ 0.    0.  ]\n",
      " [-3.07 -0.77]\n",
      " [ 4.14 -0.92]\n",
      " [-1.07  1.69]\n",
      " [ 5.6   2.38]\n",
      " [ 6.06 -2.07]\n",
      " [-3.91 -1.3 ]\n",
      " [-1.92  1.15]\n",
      " [-1.61  2.53]\n",
      " [ 7.52  1.23]\n",
      " [ 5.52 -1.23]\n",
      " [-4.76 -1.84]\n",
      " [-6.98 -2.07]\n",
      " [ 3.83 -2.3 ]]\n"
     ]
    }
   ],
   "source": [
    "factor_scores_words = cov_pca.transform(centered_words)\n",
    "print(np.round(factor_scores_words,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both of our factor scores are negative of that given in paper. It doesn't matter as principal directions are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal directions are orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.552713678800501e-15"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(factor_scores_words[:,0] * factor_scores_words[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.36,  0.18,  5.58,  0.18,  2.28,  6.34,  0.  ,  2.4 ,  4.38,\n",
       "        0.29,  8.  ,  9.37,  3.9 ,  0.94,  0.66, 14.41,  7.78,  5.77,\n",
       "       12.43,  3.75])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(factor_scores_words[:,0]**2/sum(factor_scores_words[:,0]**2)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.92,  0.55,  5.98,  0.55, 15.49,  0.28,  0.  ,  1.13,  1.63,\n",
       "        5.48, 10.87,  8.25,  3.27,  2.55, 12.32,  2.9 ,  2.9 ,  6.52,\n",
       "        8.25, 10.18])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(factor_scores_words[:,1]**2/sum(factor_scores_words[:,1]**2)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared distance to center of gravity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45.,  1., 25.,  1., 17., 25.,  0., 10., 18.,  4., 37., 41., 17.,\n",
       "        5.,  9., 58., 32., 26., 53., 20.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(factor_scores_words**2,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared cosine of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[99.  1.]\n",
      " [71. 29.]\n",
      " [88. 12.]\n",
      " [71. 29.]\n",
      " [53. 47.]\n",
      " [99.  1.]\n",
      " [nan nan]\n",
      " [94.  6.]\n",
      " [95.  5.]\n",
      " [29. 71.]\n",
      " [85. 15.]\n",
      " [90. 10.]\n",
      " [90. 10.]\n",
      " [74. 26.]\n",
      " [29. 71.]\n",
      " [97.  3.]\n",
      " [95.  5.]\n",
      " [87. 13.]\n",
      " [92.  8.]\n",
      " [74. 26.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sq_cos = factor_scores_words**2/np.sum(factor_scores_words**2, axis = 1).reshape((20,1))*100\n",
    "print(np.round(sq_cos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'nan's are produced because of division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With supplementray data\n",
    "Given a supplementary point (a point previously not used in finding principal components),we have to first center the data point. Its factor scores can then be obtained by multiplying it with the loading matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word_length     -3.0\n",
       "Lines_in_dict    4.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sur = np.array([3,12])\n",
    "sur_centered = sur - np.mean(words.iloc[:,1:3], axis = 0)\n",
    "sur_centered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor score of the new word 'sur'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.99,  0.38]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(cov_pca.transform(sur_centered.values.reshape(-1,2)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total variance before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.368"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.sum(np.var(words.values[:,1:3], axis = 0, ddof = 1)),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total variance after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.sum(cov_pca.explained_variance_),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix\n",
    "We have taken a convoluted path to find correlation matrix. But there might exist simpler ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.86790259, -0.97417637],\n",
       "       [-0.49673443, -0.22578838]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_matrix = np.repeat(np.nan, 4).reshape(2,2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        cor_matrix[i,j] = np.corrcoef(factor_scores_words[:,i], centered_words[:,j])[0,1]\n",
    "cor_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the answers for correlation coefficients donâ€™t match with that of the paper. Readers who get actual answers as given in paper are encouraged to send me an email using my contact details. However our procedure is correct and it does indeed give the correct answer for supplementary data as described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7532549 , 0.94901961],\n",
       "       [0.2467451 , 0.05098039]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(cor_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of correlation coefficients between variables and principal components is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(cor_matrix),axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading matrix matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.53687549, -0.84366149],\n",
       "       [-0.84366149, -0.53687549]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_pca.components_.T            # This is the loading matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation score for supplemenatry variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Num_entries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>700</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>700</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>500</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>900</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Frequency  Num_entries\n",
       "0           8            6\n",
       "1         230            3\n",
       "2         700           12\n",
       "3           1            2\n",
       "4         500            7\n",
       "5           1            1\n",
       "6           9            1\n",
       "7           2            6\n",
       "8           1            1\n",
       "9         700            5\n",
       "10          7            2\n",
       "11          1            1\n",
       "12          4            5\n",
       "13        500            9\n",
       "14        900            7\n",
       "15          3            1\n",
       "16          1            1\n",
       "17         10            4\n",
       "18          1            4\n",
       "19          1            2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supplementary variable (Table 4)\n",
    "Frequency = np.array([8,230,700,1,500,1,9,2,1,700,7,1,4,500,900,3,1,10,1,1])\n",
    "Num_entries = np.array([6,3,12,2,7,1,1,6,1,5,2,1,5,9,7,1,1,4,4,2])\n",
    "supp_data = pd.DataFrame({\"Frequency\" : Frequency, \"Num_entries\" : Num_entries})\n",
    "supp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered suplementary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-171.,    2.],\n",
       "       [  51.,   -1.],\n",
       "       [ 521.,    8.],\n",
       "       [-178.,   -2.],\n",
       "       [ 321.,    3.],\n",
       "       [-178.,   -3.],\n",
       "       [-170.,   -3.],\n",
       "       [-177.,    2.],\n",
       "       [-178.,   -3.],\n",
       "       [ 521.,    1.],\n",
       "       [-172.,   -2.],\n",
       "       [-178.,   -3.],\n",
       "       [-175.,    1.],\n",
       "       [ 321.,    5.],\n",
       "       [ 721.,    3.],\n",
       "       [-176.,   -3.],\n",
       "       [-178.,   -3.],\n",
       "       [-169.,    0.],\n",
       "       [-178.,    0.],\n",
       "       [-178.,   -2.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supp_data_cent = cent_data.fit_transform(supp_data.astype(np.float64))\n",
    "supp_data_cent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrlation score for supplementary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30118937, -0.69994982],\n",
       "       [ 0.72183055,  0.44928258]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_score_supp = np.repeat(np.nan,4).reshape(2,2)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        cor_score_supp[i,j] = np.corrcoef(factor_scores_words[:,i], supp_data_cent[:,j])[0,1]\n",
    "cor_score_supp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that signs of entries of this correlation matrix are different than that of the paper. This is because our factor scores are negative of the factor scores given in paper. As correlation depends on sign of input data, we get different signs for some entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared correlation for support data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09071503, 0.48992975],\n",
       "       [0.52103934, 0.20185483]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(cor_score_supp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column sums of squared correlation for support data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.61175437, 0.69178458])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(cor_score_supp), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrlation circle plot\n",
    "See the [original post](https://biswajitsahoo1111.github.io/post/principal-component-analysis-part-iii/) in R for plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "### Wine example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation PCA with wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wine_type</th>\n",
       "      <th>hedonic</th>\n",
       "      <th>for_meat</th>\n",
       "      <th>for_dessert</th>\n",
       "      <th>price</th>\n",
       "      <th>sugar</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>acidity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wine_1</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wine_2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wine_3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine_4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine_5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  wine_type  hedonic  for_meat  for_dessert  price  sugar  alcohol  acidity\n",
       "0    wine_1       14         7            8      7      7       13        7\n",
       "1    wine_2       10         7            6      4      3       14        7\n",
       "2    wine_3        8         5            5     10      5       12        5\n",
       "3    wine_4        2         4            7     16      7       11        3\n",
       "4    wine_5        6         2            4     13      3       10        3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine = pd.read_csv('D:/rfiles/data/pca_abdi/pca_abdi_wine.csv')\n",
    "wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered and scaled wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = StandardScaler()\n",
    "scaled_wine_data = scale_data.fit_transform(wine.iloc[:,1:8].astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Standard scaler centers each colulmn and divides each coulmn by its standard deviation to produce scaled data for each column. But standard deviation for each column is computed using numpy's `np.std()` command. By default `np.std()` is biased. In contrast, in **R** standard deviation is unbiased by default. So we will use unbiased standard deviation values to make scaled data in python. In this way we get same results as in `R`. This conversion is obtained by a scalar multiplication.\n",
    "\n",
    "The factor is $$\\frac{1}{\\sqrt{\\frac{N}{N-1}}}$$ Where $N$ is number of data points. In our case $N=5$ as there are five wine types in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_wine_data = scaled_wine_data* (1/np.sqrt(5/4))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on scaled wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine_cor = PCA().fit(scaled_wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_wine = pca_wine_cor.transform(scaled_wine_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.3302,  1.0953],\n",
       "       [-2.0842, -1.2232],\n",
       "       [ 0.1673, -0.3703],\n",
       "       [ 1.7842,  1.7126],\n",
       "       [ 2.4628, -1.2144]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(transformed_data_wine[:,0:2],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of each observation to principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28.5 , 16.57],\n",
       "       [22.8 , 20.66],\n",
       "       [ 0.15,  1.89],\n",
       "       [16.71, 40.51],\n",
       "       [31.84, 20.37]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(transformed_data_wine[:,0:2]**2/np.sum(transformed_data_wine[:,0:2]**2, axis = 0 )*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared cosine of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[77., 17.],\n",
       "       [69., 24.],\n",
       "       [ 7., 34.],\n",
       "       [50., 46.],\n",
       "       [78., 19.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(transformed_data_wine[:,0:2]**2/np.sum(transformed_data_wine**2, axis = 1).reshape(5,1)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading scores corresponding to first two principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4 , -0.11],\n",
       "       [-0.45,  0.11],\n",
       "       [-0.26,  0.59],\n",
       "       [ 0.42,  0.31],\n",
       "       [-0.05,  0.72],\n",
       "       [-0.44, -0.06],\n",
       "       [-0.45, -0.09]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pca_wine_cor.components_[0:2,:].T,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation score of variables with first two principal components\n",
    "Rows correspond to PC1 and PC2 respectively and columns correspond to featuers in wine data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87, -0.97, -0.58,  0.91, -0.11, -0.96, -0.99],\n",
       "       [-0.15,  0.15,  0.79,  0.42,  0.97, -0.07, -0.12]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_score_wine = np.repeat(np.nan,14).reshape(2,7)\n",
    "for i in range(2):\n",
    "    for j in range(7):\n",
    "        cor_score_wine[i,j] = np.corrcoef(transformed_data_wine[:,i], wine.values[:,j+1].astype(np.float64))[0,1]\n",
    "np.round(cor_score_wine,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varimax roation\n",
    "\n",
    "There is no function in python that performs varimax rotation. However, there are public codes to perform it in python. We use the code from this [Wikipedia page](https://en.wikipedia.org/wiki/Talk:Varimax_rotation). The function below is exact copy of the funciton in that page with minor changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def varimax(Phi, gamma = 1.0, q = 20, tol = 1e-6):\n",
    "    import numpy as np\n",
    "    from scipy import eye, asarray, dot, sum\n",
    "    from scipy.linalg import svd\n",
    "    p,k = Phi.shape\n",
    "    R = eye(k)\n",
    "    d=0\n",
    "    for i in np.arange(q):\n",
    "        d_old = d\n",
    "        Lambda = dot(Phi, R)\n",
    "        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, np.diag(np.diag(dot(Lambda.T,Lambda))))))\n",
    "        R = dot(u,vh)\n",
    "        d = sum(s)\n",
    "        if d_old!=0 and d/d_old < 1 + tol: break\n",
    "    return dot(Phi, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute rotated scores for first two columns of loading matrix that correspond to first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41, -0.03],\n",
       "       [-0.41,  0.2 ],\n",
       "       [-0.14,  0.63],\n",
       "       [ 0.47,  0.22],\n",
       "       [ 0.1 ,  0.72],\n",
       "       [-0.44,  0.04],\n",
       "       [-0.46,  0.01]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(varimax(pca_wine_cor.components_.T[:,0:2]),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above result, rows correspond to features in wine data. First column is the modified loading scores corresponding to first principal component and similarly for second column. Note that this result doesn't match with the paper. Neither does it match with `R` results. But it is close to the results given in paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3\n",
    "### French food example (Covariance PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>children</th>\n",
       "      <th>bread</th>\n",
       "      <th>vegetables</th>\n",
       "      <th>fruit</th>\n",
       "      <th>meat</th>\n",
       "      <th>poultry</th>\n",
       "      <th>milk</th>\n",
       "      <th>wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue_collar</td>\n",
       "      <td>2</td>\n",
       "      <td>332</td>\n",
       "      <td>428</td>\n",
       "      <td>354</td>\n",
       "      <td>1437</td>\n",
       "      <td>526</td>\n",
       "      <td>247</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>White_collar</td>\n",
       "      <td>2</td>\n",
       "      <td>293</td>\n",
       "      <td>559</td>\n",
       "      <td>388</td>\n",
       "      <td>1527</td>\n",
       "      <td>567</td>\n",
       "      <td>239</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Upper_class</td>\n",
       "      <td>2</td>\n",
       "      <td>372</td>\n",
       "      <td>767</td>\n",
       "      <td>562</td>\n",
       "      <td>1948</td>\n",
       "      <td>927</td>\n",
       "      <td>235</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Blue_collar</td>\n",
       "      <td>3</td>\n",
       "      <td>406</td>\n",
       "      <td>563</td>\n",
       "      <td>341</td>\n",
       "      <td>1507</td>\n",
       "      <td>544</td>\n",
       "      <td>324</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White_collar</td>\n",
       "      <td>3</td>\n",
       "      <td>386</td>\n",
       "      <td>608</td>\n",
       "      <td>396</td>\n",
       "      <td>1501</td>\n",
       "      <td>558</td>\n",
       "      <td>319</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Upper_class</td>\n",
       "      <td>3</td>\n",
       "      <td>438</td>\n",
       "      <td>843</td>\n",
       "      <td>689</td>\n",
       "      <td>2345</td>\n",
       "      <td>1148</td>\n",
       "      <td>243</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Blue_collar</td>\n",
       "      <td>4</td>\n",
       "      <td>534</td>\n",
       "      <td>660</td>\n",
       "      <td>367</td>\n",
       "      <td>1620</td>\n",
       "      <td>638</td>\n",
       "      <td>414</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>White_collar</td>\n",
       "      <td>4</td>\n",
       "      <td>460</td>\n",
       "      <td>699</td>\n",
       "      <td>484</td>\n",
       "      <td>1856</td>\n",
       "      <td>762</td>\n",
       "      <td>400</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Upper_class</td>\n",
       "      <td>4</td>\n",
       "      <td>385</td>\n",
       "      <td>789</td>\n",
       "      <td>621</td>\n",
       "      <td>2366</td>\n",
       "      <td>1149</td>\n",
       "      <td>304</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blue_collar</td>\n",
       "      <td>5</td>\n",
       "      <td>655</td>\n",
       "      <td>776</td>\n",
       "      <td>423</td>\n",
       "      <td>1848</td>\n",
       "      <td>759</td>\n",
       "      <td>495</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>White_collar</td>\n",
       "      <td>5</td>\n",
       "      <td>584</td>\n",
       "      <td>995</td>\n",
       "      <td>548</td>\n",
       "      <td>2056</td>\n",
       "      <td>893</td>\n",
       "      <td>518</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Upper_class</td>\n",
       "      <td>5</td>\n",
       "      <td>515</td>\n",
       "      <td>1097</td>\n",
       "      <td>887</td>\n",
       "      <td>2630</td>\n",
       "      <td>1167</td>\n",
       "      <td>561</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class  children  bread  vegetables  fruit  meat  poultry  milk  \\\n",
       "0    Blue_collar         2    332         428    354  1437      526   247   \n",
       "1   White_collar         2    293         559    388  1527      567   239   \n",
       "2    Upper_class         2    372         767    562  1948      927   235   \n",
       "3    Blue_collar         3    406         563    341  1507      544   324   \n",
       "4   White_collar         3    386         608    396  1501      558   319   \n",
       "5    Upper_class         3    438         843    689  2345     1148   243   \n",
       "6    Blue_collar         4    534         660    367  1620      638   414   \n",
       "7   White_collar         4    460         699    484  1856      762   400   \n",
       "8    Upper_class         4    385         789    621  2366     1149   304   \n",
       "9    Blue_collar         5    655         776    423  1848      759   495   \n",
       "10  White_collar         5    584         995    548  2056      893   518   \n",
       "11   Upper_class         5    515        1097    887  2630     1167   561   \n",
       "\n",
       "    wine  \n",
       "0    427  \n",
       "1    258  \n",
       "2    433  \n",
       "3    407  \n",
       "4    363  \n",
       "5    341  \n",
       "6    407  \n",
       "7    416  \n",
       "8    282  \n",
       "9    486  \n",
       "10   319  \n",
       "11   284  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food = pd.read_csv('D:/rfiles/data/pca_abdi/pca_abdi_food.csv')\n",
    "food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_cov = PCA().fit(food.iloc[:,2:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor scores\n",
    "`sklearn`'s PCA centers data by default before applying the algorithm. We can also center data manually before giving it as an argument to fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-635.05, -120.89],\n",
       "       [-488.56, -142.33],\n",
       "       [ 112.03, -139.75],\n",
       "       [-520.01,   12.05],\n",
       "       [-485.94,    1.17],\n",
       "       [ 588.17, -188.44],\n",
       "       [-333.95,  144.54],\n",
       "       [ -57.51,   42.86],\n",
       "       [ 571.32, -206.76],\n",
       "       [ -39.38,  264.47],\n",
       "       [ 296.04,  235.92],\n",
       "       [ 992.83,   97.15]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factor_scores_food = food_cov.transform(food.iloc[:,2:9])\n",
    "np.round(factor_scores_food[:,0:2],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contribution of each observation to principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.34,  5.03],\n",
       "       [ 7.9 ,  6.97],\n",
       "       [ 0.42,  6.72],\n",
       "       [ 8.94,  0.05],\n",
       "       [ 7.81,  0.  ],\n",
       "       [11.44, 12.22],\n",
       "       [ 3.69,  7.19],\n",
       "       [ 0.11,  0.63],\n",
       "       [10.8 , 14.71],\n",
       "       [ 0.05, 24.07],\n",
       "       [ 2.9 , 19.15],\n",
       "       [32.61,  3.25]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(factor_scores_food[:,0:2]**2/np.sum(factor_scores_food[:,0:2]**2, axis = 0)*100,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared cosine of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 95.,   3.],\n",
       "       [ 86.,   7.],\n",
       "       [ 26.,  40.],\n",
       "       [100.,   0.],\n",
       "       [ 98.,   0.],\n",
       "       [ 89.,   9.],\n",
       "       [ 83.,  15.],\n",
       "       [ 40.,  22.],\n",
       "       [ 86.,  11.],\n",
       "       [  2.,  79.],\n",
       "       [ 57.,  36.],\n",
       "       [ 97.,   1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(factor_scores_food[:,0:2]**2/np.sum(factor_scores_food**2, axis = 1).reshape(12,1)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared loading scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.33],\n",
       "       [0.11, 0.17],\n",
       "       [0.09, 0.01],\n",
       "       [0.57, 0.01],\n",
       "       [0.22, 0.06],\n",
       "       [0.01, 0.4 ],\n",
       "       [0.  , 0.02]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(food_cov.components_.T[:,0:2]**2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this answer doesn't match with the answers given in paper (Table 13). But we will stick with our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36,  0.91,  0.96,  1.  ,  0.98,  0.41, -0.43],\n",
       "       [ 0.87,  0.35, -0.1 , -0.04, -0.16,  0.88,  0.33]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_score_food = np.repeat(np.nan,14).reshape(2,7)\n",
    "for i in range(2):\n",
    "    for j in range(7):\n",
    "        cor_score_food[i,j] = np.corrcoef(factor_scores_food[:,i], food.values[:,j+2].astype(np.float64))[0,1]\n",
    "np.round(cor_score_food,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above answer, each column corresponds to a feature in food data excluding class and children. The two rows correspond to first two principal components. Second row of above answer also differs in sign from the results of `R`. This is due to the fact that second principal component factor scores of `R` are negative of python's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared correlation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13, 0.83, 0.92, 1.  , 0.96, 0.17, 0.18],\n",
       "       [0.76, 0.12, 0.01, 0.  , 0.03, 0.77, 0.11]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(cor_score_food**2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen values of data covariance matrix is square of singular values of centered data matrix. Hence eigen values of data covariance matrix can be obtained as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3023141.24],\n",
       "       [ 290575.84],\n",
       "       [  68795.23],\n",
       "       [  25298.95],\n",
       "       [  22992.25],\n",
       "       [   3722.32],\n",
       "       [    723.92]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigen_values = food_cov.singular_values_.reshape(7,1)**2\n",
    "np.set_printoptions(suppress= True)\n",
    "np.round(eigen_values,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage contribution of each principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88],\n",
       "       [0.08],\n",
       "       [0.02],\n",
       "       [0.01],\n",
       "       [0.01],\n",
       "       [0.  ],\n",
       "       [0.  ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(eigen_values/np.sum(eigen_values),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative sum of eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3023141.24],\n",
       "       [3313717.07],\n",
       "       [3382512.31],\n",
       "       [3407811.26],\n",
       "       [3430803.5 ],\n",
       "       [3434525.83],\n",
       "       [3435249.75]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.cumsum(eigen_values).reshape(7,1),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative percentage contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88, 0.96, 0.98, 0.99, 1.  , 1.  , 1.  ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.cumsum(eigen_values)/np.sum(eigen_values),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESS (Refer to the paper for a description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[412108.51],\n",
       "       [121532.68],\n",
       "       [ 52737.44],\n",
       "       [ 27438.49],\n",
       "       [  4446.25],\n",
       "       [   723.92],\n",
       "       [     0.  ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESS = np.repeat(np.nan, 7)\n",
    "for i in range(7):\n",
    "    RESS[i] = np.sum(eigen_values) - np.sum(eigen_values[:i+1])\n",
    "np.round(RESS.reshape(7,1),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ration of RESS and sum of eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12, 0.04, 0.02, 0.01, 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(RESS/np.sum(eigen_values),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not calculated the value of PRESS in this post as it will require us to consider random models. We will not pursue it here. Comments regarding any errors or omissions may be sent to [the author's](https://biswajitsahoo1111.github.io/) email."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflwo_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
